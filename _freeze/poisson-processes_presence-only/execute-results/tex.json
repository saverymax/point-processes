{
  "hash": "e5d52b24275416fcf653e775a15490c1",
  "result": {
    "markdown": "# Data generation and Modelling\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(viridis)\nlibrary(hrbrthemes)\nlibrary(cmdstanr) \nlibrary(bayesplot)\nlibrary(spatstat)\nlibrary(dplyr)\nlibrary(tidyr)\n\nsource(\"presence_only_functions.R\")\nsource(\"surface_functions.R\")\neval_pp <- T\neval_lgcp <- F\n```\n:::\n\n\n## Implementation\n\nThe previous sections discussed the theoretical and practical aspects of data generation. We now proceed to implement the data generation procedure. We first generate observations at each site and create plots of observed intensities, bias, and counts. We then fit a Poisson Point Process to the generated data.\n\nWe create a ring-shaped environment based on two correlated covariates and generate PO points accordingly.\n\n\n::: {.cell}\n\n```{.r .cell-code}\narea_D <- 100\nk <- 20\nsites <- k^2\n# Parameter values somewhere around Fithian 2015.\nalpha <- -2 \nbeta <- 2\ngamma <- -2\ndelta <- 0.5\nparams <- list(alpha=alpha, beta=beta, gamma=gamma, delta=delta)\naux_cor <- 0.7\ngrid_points <- get_sampling_surface_donut(k)\n```\n\n::: {.cell-output-display}\n![](poisson-processes_presence-only_files/figure-pdf/data-gen-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\ngrid_points <- get_bias_surface_correlated(grid_points, aux_cor)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Correlation between X and Z\"\n          [,1]      [,2]\n[1,] 1.0000000 0.7976719\n[2,] 0.7976719 1.0000000\n```\n:::\n\n::: {.cell-output-display}\n![](poisson-processes_presence-only_files/figure-pdf/data-gen-2.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nsites <- sites/4\ngrid_points <- grid_points %>% dplyr::filter(x<(k/2)+1, y<(k/2)+1)\n# Filter for only a quarter of the grid.\n# If we filter, we need to change the total sites as well\n# Need distances for LGCP\ndistance_mat <- as.matrix(dist(grid_points[,1:2]))\ndata_reps <- 1\ncorr_matrix <- specify_corr(grid_points[,1:2])\ngp_bool <- F\nsim_data <- generate_ppp_data_r(grid_points, params, sites, data_reps, corr_matrix, gp_bool, area_D)\nY_positive_indices <- which(sim_data$Y>0)\nstopifnot(sum(sim_data$Y[Y_positive_indices]) == nrow(sim_data$coordinates))\nsim_data$Y_coords\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           x         y\n1   8.936481  2.741761\n2   9.209482  3.204360\n3   9.229643  2.981074\n4   9.152395  3.463423\n5  10.051808  2.866581\n6  10.053061  3.403884\n7   6.378491  4.218996\n8   5.002833  4.888674\n9   5.477546  5.063880\n10  5.396063  4.738356\n11  4.781126  5.374346\n12  3.572140  5.563733\n13  4.061799  5.856083\n14  3.257821  9.468339\n15  2.935217  8.671601\n16  3.254904  9.129159\n17  2.995517  8.903966\n18  3.187029 10.152890\n19  3.461228  9.813877\n```\n:::\n\n```{.r .cell-code}\ngrid_points[Y_positive_indices,]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    x  y    aux_x     aux_z\n29  9  3 2.257839 0.5712682\n30 10  3 2.326348 1.3443836\n36  6  4 1.921817 0.9256167\n45  5  5 2.257839 0.9596080\n54  4  6 1.921817 0.9207442\n83  3  9 2.257839 1.8952758\n93  3 10 2.326348 0.4309095\n```\n:::\n:::\n\n\nThe above output shows the exact location of the generated points and the associated $x$ and $z$ covariate values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Use the final generation iteration\np <- ggplot(grid_points, aes(x, y, fill=sim_data$lambda[data_reps,])) + \n  geom_tile() +\n  scale_fill_viridis(discrete=FALSE) +\n  ggtitle(\"Generated intensity per site\")\nprint(p)\n```\n\n::: {.cell-output-display}\n![](poisson-processes_presence-only_files/figure-pdf/plot-points-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\np <- ggplot(grid_points, aes(x, y, fill=sim_data$bias[data_reps,])) + \n  geom_tile() +\n  scale_fill_viridis(discrete=FALSE) +\n  ggtitle(\"Generated bias per site\")\nprint(p)\n```\n\n::: {.cell-output-display}\n![](poisson-processes_presence-only_files/figure-pdf/plot-points-2.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nthinned_intensity <- sim_data$lambda[data_reps,]*sim_data$bias[data_reps,]\np <- ggplot() +\n  geom_tile(grid_points, mapping=aes(x, y, fill=thinned_intensity, width=1, height=1), alpha=.6) + \n  scale_fill_viridis(discrete=FALSE, name=\"L*b\") +\n  ggtitle(\"Generated thinning per site, including generated (or observed) individuals\") +\n  geom_point(data=sim_data$Y_coords, mapping=aes(x=x, y=y), size=3, col=\"white\") +\n  theme(panel.grid.minor = element_line(colour=\"white\")) +\n  scale_y_continuous(breaks = seq(0, 20, 1)) +\n  scale_x_continuous(breaks = seq(0, 20, 1)) \nprint(p)\n```\n\n::: {.cell-output-display}\n![](poisson-processes_presence-only_files/figure-pdf/plot-points-3.pdf){fig-pos='H'}\n:::\n:::\n\n\nThe above plots show the PO points and the ring-shaped surface.\n\n\n\n\n\n\nNext we fit the Thinned Nonhomogenous Poisson Process model in Stan. We can make a few observations about this model. First, examine the likelihood: \n$$L(\\lambda|Y) = \\prod^{N}_{i=1}\\frac{\\lambda_i^{n_i}\\exp[-\\lambda_i]}{n_i!}$$. This is made using  the assumption that the counts at sites $s_i$ are distributed following a poisson process with mean $\\lambda$. This assumption allows for a simpler specification of the likelihood, since we can use the poisson distribution. The setting of $|A|=1$ allows us to make this assumption, as we do not need to multiple the intensity parameter $\\lambda$ by the area in the model.\n\nThe next sections of code fitting and diagnosing the poisson process model in Stan, using the generated data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_string <- stan_poisson_process\nwrite(model_string, model_path)\n# data_reps is 1 here so just use the first index\ndata = list(N = sites, X = grid_points$aux_x, Z = grid_points$aux_z, y = sim_data$Y[data_reps,])\nmodel <- cmdstan_model(model_path)\nfit <- model$sample(data=data, seed=13, chains=3, iter_sampling=2000, iter_warmup=200)\n```\n:::\n\n\nThe above code fits the table. We next examine the fit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit$summary(variables=c('alpha', 'beta', 'gamma', 'delta'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 x 10\n  variable   mean median    sd   mad      q5   q95  rhat ess_bulk ess_tail\n  <chr>     <dbl>  <dbl> <dbl> <dbl>   <dbl> <dbl> <dbl>    <dbl>    <dbl>\n1 alpha    -4.93  -4.88  7.12  7.16  -16.7   6.65   1.00    2376.    2563.\n2 beta      4.50   4.39  1.17  1.16    2.83  6.58   1.00    2681.    2228.\n3 gamma    -4.39  -4.51  7.22  7.13  -16.2   7.65   1.00    2398.    2656.\n4 delta     0.141  0.137 0.416 0.411  -0.542 0.825  1.00    3847.    3211.\n```\n:::\n:::\n\n\nFrom the table above, we can observe that the parameters $\\beta$ and $\\delta$ have been recovered. However, due to the correlation of $\\alpha$ and $\\gamma$, only the sum $\\alpha+\\gamma$ can be correctly identified. Our \"true\" sum of was equal to -4, and we can see the estimate here is $~4.7$, which is reasonable given we incorporate priors and are estimating correlated parameters. The correlation between the two can be observed in the scatter plots below, as well as in the wide confidence interval of the posterior, relative to the other parameters.\n\n## Diagnostics\n\nHere we check goodness of fit and Posterior Predictive Checks.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparam_vec <- c('alpha', 'gamma', 'beta', 'delta')\nposterior <- fit$draws(variables=param_vec)\ncolor_scheme_set(\"mix-blue-pink\")\np_trace <- mcmc_trace(posterior,  pars = c(\"alpha\", \"beta\", \"gamma\", \"delta\"),\n                      facet_args = list(nrow = 2, labeller = label_parsed))\nprint(p_trace + facet_text(size = 15))\n```\n\n::: {.cell-output-display}\n![](poisson-processes_presence-only_files/figure-pdf/posterior-diagnostics-pp-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nplot_title <- ggtitle(paste(\"Posterior distributions, with means and 90% interval\"))\np_post <- mcmc_areas(posterior,  prob = 0.9, point_est=\"mean\", regex_pars = c(\"alpha\", \"beta\")) + plot_title\nprint(p_post)\n```\n\n::: {.cell-output-display}\n![](poisson-processes_presence-only_files/figure-pdf/posterior-diagnostics-pp-2.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nplot_title <- ggtitle(paste(\"Posterior distributions, with means and 90% interval\"))\np_post <- mcmc_areas(posterior,  prob = 0.9, point_est=\"mean\", regex_pars = c(\"gamma\", \"delta\")) + plot_title\nprint(p_post)\n```\n\n::: {.cell-output-display}\n![](poisson-processes_presence-only_files/figure-pdf/posterior-diagnostics-pp-3.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nmcmc_intervals(posterior)\n```\n\n::: {.cell-output-display}\n![](poisson-processes_presence-only_files/figure-pdf/posterior-diagnostics-pp-4.pdf){fig-pos='H'}\n:::\n:::\n\n\nImportantly, note the wide credible intervals on $\\alpha$ and $\\gamma$. Why? See the scatter plots below.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_title <- ggtitle(\"Parameter posterior sample correlation\")\np_post <- mcmc_pairs(posterior)\nprint(p_post)\n```\n\n::: {.cell-output-display}\n![](poisson-processes_presence-only_files/figure-pdf/param-corr-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nmcmc_scatter(posterior, pars=c('alpha','gamma'))\n```\n\n::: {.cell-output-display}\n![](poisson-processes_presence-only_files/figure-pdf/param-corr-2.pdf){fig-pos='H'}\n:::\n:::\n\n\nThere is near perfect correlation between $\\alpha$ and $\\gamma$, as expected.\n\n## PPD\n\nWe next look at the Posterior Predictive Distribution (PPD) of the observations per site. That is, we plot the generated counts $Y_i$ for site $s_i$ on the sampling surface (grid). \n\nThe PPD can be expressed as the samples from the predictive distribution of $Y$ taking into account the uncertainty surrounding the parameters. The PPD can be written as\n$$\np(\\tilde{y}|Y) = \\int p(\\tilde{y}|\\lambda, b, Y)p(\\lambda, b|Y) dbd\\lambda\n$$\n\nsuch that the posterior uncertainty of the parameters is integrated out.\n\nBelow we plot the generated values of $\\tilde{y}$ for each site. Additionally, the generated quantities of intensity, bias, and intensity*bias ($\\lambda\\cdot b) per site are also plotted on the grid. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngenerated_vars <- c('lambda_bias', 'lambda_rep', 'b_rep', 'y_rep')\nlambda_thinned <- fit$summary(variables=generated_vars[1])$mean\nlambda_rep <- fit$summary(variables=generated_vars[2])$mean\nbias_rep <- fit$summary(variables=generated_vars[3])$mean\ny_ppd <- fit$summary(variables=generated_vars[4])$mean\nppd_df <- data.frame(x=grid_points$x, y=grid_points$y, y_rep=y_ppd, lt=lambda_thinned, l=lambda_rep, b=bias_rep)\nhead(ppd_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  x y y_rep           lt         l        b\n1 1 1     0 2.265085e-06  117.7815 687943.3\n2 2 1     0 2.269410e-06  118.3296 683279.4\n3 3 1     0 2.690398e-06  124.3014 819107.3\n4 4 1     0 2.821546e-06  165.6207 687975.1\n5 5 1     0 4.762552e-06  387.2346 676181.2\n6 6 1     0 1.052730e-05 1521.8531 643690.4\n```\n:::\n:::\n\n\nIt is important to notice that while the thinned intensity is ostensibly realistic in estimation, the intensity and bias are not. This is due to the specification of the Thinned Poisson Process\n$$\\lambda^* = \\lambda(s)b(s)$$\nsuch that\n$$\\lambda^* = \\exp\\left[\\alpha + \\beta*X(s) + \\gamma + \\delta*Z(s)\\right]$$\n\nWe can then see the generated data in the sampling surface:\n\n\n::: {.cell}\n\n```{.r .cell-code}\np <- ggplot(ppd_df, aes(x, y, fill=y_rep)) + \n    geom_tile() +\n    scale_fill_viridis(discrete=FALSE) +\n    ggtitle(\"PPD counts per site\")\nprint(p)\n```\n\n::: {.cell-output-display}\n![](poisson-processes_presence-only_files/figure-pdf/ppd-plots-pp-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\np <- ggplot(ppd_df, aes(x, y, fill=lt)) + \n    geom_tile() +\n    scale_fill_viridis(discrete=FALSE) +\n    ggtitle(\"Thinned intensity per site\")\nprint(p)\n```\n\n::: {.cell-output-display}\n![](poisson-processes_presence-only_files/figure-pdf/ppd-plots-pp-2.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\np <- ggplot(ppd_df, aes(x, y, fill=l)) + \n    geom_tile() +\n    scale_fill_viridis(discrete=FALSE) +\n    ggtitle(\"Intensity per site\")\nprint(p)\n```\n\n::: {.cell-output-display}\n![](poisson-processes_presence-only_files/figure-pdf/ppd-plots-pp-3.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\np <- ggplot(ppd_df, aes(x, y, fill=b)) + \n    geom_tile() +\n    scale_fill_viridis(discrete=FALSE) +\n    ggtitle(\"Bias per site\")\nprint(p)\n```\n\n::: {.cell-output-display}\n![](poisson-processes_presence-only_files/figure-pdf/ppd-plots-pp-4.pdf){fig-pos='H'}\n:::\n:::\n\n\nThe prediction of PO observatiosn at each site is realistic, as is the thinned intensity. But alone, the intensity and bias predictive disitributions are poorly estimated.\n\n\n## Correlation between coefficients in Thinned poisson process model for presence-only data.\nEDIT FROM HERE: want to say something slightly different.\nIn the above plots, we can notice that the bias and (unthinned) intensity are behaving strangely. Here we observe the effect of correlated parameters describing the behavior of the intensity and the bias the NHPP. We examine the distributions of the generated quantities using the fitted model.\n\nThe issue with the large values is both because we are taking the mean of the posterior distribution as our point estimate, and the high correlation between the two parameters causes the estimates to inversely fluctuate around each other.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_title <- ggtitle(paste(\"Posterior distributions, with means and 90% interval\"))\np_post <- mcmc_intervals(fit$draws(),  prob = 0.9, point_est=\"mean\", regex_pars = generated_vars[1]) + plot_title\nprint(p_post)\n```\n\n::: {.cell-output-display}\n![](poisson-processes_presence-only_files/figure-pdf/generated-params-pp-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\np_post <- mcmc_intervals(fit$draws(),  prob = 0.9, point_est=\"mean\", regex_pars = generated_vars[2]) + plot_title\nprint(p_post)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 25 rows containing missing values (`geom_point()`).\n```\n:::\n\n::: {.cell-output-display}\n![](poisson-processes_presence-only_files/figure-pdf/generated-params-pp-2.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\np_post <- mcmc_intervals(fit$draws(),  prob = 0.9, point_est=\"mean\", regex_pars = generated_vars[3]) + plot_title\nprint(p_post)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 100 rows containing missing values (`geom_point()`).\n```\n:::\n\n::: {.cell-output-display}\n![](poisson-processes_presence-only_files/figure-pdf/generated-params-pp-3.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n# Look at counts for each site\ngen_y <- c(\"y_rep[1]\", \"y_rep[10]\", \"y_rep[20]\")\np_post <- mcmc_hist(fit$draws(), pars=gen_y)\nprint(p_post)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](poisson-processes_presence-only_files/figure-pdf/generated-params-pp-4.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n# The gen parameters for just a few of the sites\ngen_lambda <- c('lambda_rep[1]', 'lambda_rep[10]', 'lambda_rep[20]')\ngen_lb <- c('lambda_bias[1]', 'lambda_bias[10]', 'lambda_bias[20]')\ngen_b <- c('b_rep[1]', 'b_rep[10]', 'b_rep[20]')\np_post <- mcmc_intervals(fit$draws(),  prob = 0.9, point_est=\"mean\", pars = gen_lambda) + plot_title\nprint(p_post)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n```\n:::\n\n::: {.cell-output-display}\n![](poisson-processes_presence-only_files/figure-pdf/generated-params-pp-5.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\np_post <- mcmc_hist(fit$draws(), pars = gen_lambda) \nprint(p_post)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](poisson-processes_presence-only_files/figure-pdf/generated-params-pp-6.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\np_post <- mcmc_hist(fit$draws(), pars = gen_b)\nprint(p_post)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](poisson-processes_presence-only_files/figure-pdf/generated-params-pp-7.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\np_post <- mcmc_intervals(fit$draws(),  prob = 0.9, point_est=\"mean\", pars = gen_lb) + plot_title\nprint(p_post)\n```\n\n::: {.cell-output-display}\n![](poisson-processes_presence-only_files/figure-pdf/generated-params-pp-8.pdf){fig-pos='H'}\n:::\n:::\n\n\nThe take away is that the mean for the generated quantities is very skewed by the occassional large value. This high skew is caused by the correlation between b and l, since these parameters are not estimable between themselves. Therefore, we arrive at reasonable estimates  for b*l since this is draw specific (where when one is high the other is low), but  the overall mean for either b or l over multiple draws cannot be trusted. This is clear from the histograms of $\\lambda$ and the bias, where most of the values take on very low values, but do fluctuate higher. And when one goes higher, the other goes lower, which I'd like to check a specific example of, for one draw.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Nuts diagnostics that I haven't tried to understand really\nnuts_fit <- nuts_params(fit)\nhead(nuts_fit)\nmcmc_nuts_treedepth(nuts_fit, log_posterior(fit))\nmcmc_nuts_acceptance(nuts_fit,  log_posterior(fit))\nmcmc_parcoord(fit$  draws(), np=nuts_fit)\nmcmc_violin(fit$draws(), pars = c('alpha', 'beta', 'gamma', 'delta'))\nmcmc_nuts_divergence(nuts_fit, log_posterior(fit))\nmcmc_nuts_energy(nuts_fit, binwidth=1/2)\nmcmc_rhat(rhat(fit))\nmcmc_neff(neff_ratio(fit))\n```\n:::\n::: {.cell}\n\n```{.r .cell-code}\nmcmc_acf(fit$draws(), pars=c('alpha', 'beta', 'gamma', 'delta'))\n```\n\n::: {.cell-output-display}\n![](poisson-processes_presence-only_files/figure-pdf/acf-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nAutocorrelation in the chains looks well-handled.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit$cmdstan_diagnose()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProcessing csv files: C:/Users/msavery/AppData/Local/Temp/RtmpMT52Tm/thinned_poisson_process-202404291032-1-119bb6.csv, C:/Users/msavery/AppData/Local/Temp/RtmpMT52Tm/thinned_poisson_process-202404291032-2-119bb6.csv, C:/Users/msavery/AppData/Local/Temp/RtmpMT52Tm/thinned_poisson_process-202404291032-3-119bb6.csv\n\nChecking sampler transitions treedepth.\nTreedepth satisfactory for all transitions.\n\nChecking sampler transitions for divergences.\nNo divergent transitions found.\n\nChecking E-BFMI - sampler transitions HMC potential energy.\nE-BFMI satisfactory.\n\nEffective sample size satisfactory.\n\nSplit R-hat values satisfactory all parameters.\n\nProcessing complete, no problems detected.\n```\n:::\n\n```{.r .cell-code}\n# Leave loo out.\n#loo_results <- fit$loo(variables=\"lp__\", cores=4)\n#print(loo_results)\n```\n:::\n\n\nIt appears we have no additional problems in the models according the the Stan diagnostics.\n\nWe next move on to incorporating spatial correlation. To do so, we fit a Log Gaussian Cox Process (LGCP). Where before we specified the intensity as\n$$\n\\lambda(s) = \\theta(s)b(s) = \\exp[\\alpha + \\beta'x(s) + \\gamma + \\delta'z(s)]\n$$\nwe now add a Gaussian process to incorporate spatial correlation between sites\n$$\n\\lambda(s) = \\theta(s)b(s) = \\exp[\\alpha + \\beta'x(s) + \\gamma + \\delta'z(s) + w(s)]\n$$\nso that the expected value for quadrat $A$ will be the integral over the quadrat:\n$$\n\\Delta(A) = \\int_A\\lambda(s)ds = \\int_A \\exp[\\alpha + \\beta'x(s) + \\gamma + \\delta'z(s) + w(s)]ds\n$$\nWe make the same approximations as before based on our limited covariate resolution. \n\nWhile the topic of LGCPs on its own is quite interesting and deserves its own treatment, for now we examine its behavior only as an adjustment to the NHPP. In a future post I will discuss approximations to the LGCP, which are quite important as fitting the spatial correlation matrix is computationally intensive and not practical if we will want to apply the LGCP to developing optimal designs.\n\nI proceed with the stan code.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_string <- log_gaussian_cox_process\nwrite(model_string, model_path)\n# Here we need to use the distance matrix\ndata = list(N = sites, X = grid_points$aux_x, Z = grid_points$aux_z, y = sim_data$Y[data_reps,], D = distance_mat)\nmodel <- cmdstan_model(model_path)\nlgcp_fit <- model$sample(data=data, seed=13, chains=3, iter_sampling=2000, iter_warmup=200, thin = 10)\n```\n:::\n\n\n\nHaving fit the LGCP we can examine the parameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlgcp_fit$summary(variables=c('alpha', 'beta', 'gamma', 'delta'))\n```\n:::\n\n\nAnd then we look at the posterior diagnostics.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior <- lgcp_fit$draws()\ncolor_scheme_set(\"mix-blue-pink\")\np_trace <- mcmc_trace(posterior,  pars = c(\"alpha\", \"beta\", \"gamma\", \"delta\"),\n                      facet_args = list(nrow = 2, labeller = label_parsed))\nprint(p_trace + facet_text(size = 15))\n\nplot_title <- ggtitle(paste(\"Posterior distributions, with means and 90% interval\"))\np_post <- mcmc_areas(posterior,  prob = 0.9, point_est=\"mean\", regex_pars = c(\"alpha\", \"beta\")) + plot_title\nprint(p_post)\n\nplot_title <- ggtitle(paste(\"Posterior distributions, with means and 90% interval\"))\np_post <- mcmc_areas(posterior,  prob = 0.9, point_est=\"mean\", regex_pars = c(\"gamma\", \"delta\")) + plot_title\nprint(p_post)\n\nmcmc_intervals(posterior, pars=c('alpha', 'beta', 'gamma', 'delta'))\n#mcmc_hist(fit$draws(), pars = c('alpha', 'beta', 'gamma', 'delta')) \nplot_title <- ggtitle(\"Parameter posterior sample correlation\")\np_post <- mcmc_pairs(posterior, pars=c('alpha', 'beta', 'gamma', 'delta'))\nprint(p_post)\nmcmc_scatter(posterior, pars=c('alpha','gamma'))\n```\n:::\n\n\nFinally look at the posterior predictions for $\\lambda$, $b$, and $\\lambda\\cdot b$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngenerated_vars <- c('lambda_bias', 'lambda_rep', 'b_rep', 'y_rep')\nlambda_thinned <- lgcp_fit$summary(variables=generated_vars[1])$mean\nlambda_rep <- lgcp_fit$summary(variables=generated_vars[2])$mean\nbias_rep <- lgcp_fit$summary(variables=generated_vars[3])$mean\ny_ppd <- lgcp_fit$summary(variables=generated_vars[4])$mean\nppd_df <- data.frame(x=grid_points$x, y=grid_points$y, y_rep=y_ppd, lt=lambda_thinned, l=lambda_rep, b=bias_rep)\nhead(ppd_df)\n```\n:::\n\n\nWe can then check the generated data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\np <- ggplot(ppd_df, aes(x, y, fill=y_rep)) + \n    geom_tile() +\n    scale_fill_viridis(discrete=FALSE) +\n    ggtitle(\"PPD counts per site\")\nprint(p)\n\np <- ggplot(ppd_df, aes(x, y, fill=lt)) + \n    geom_tile() +\n    scale_fill_viridis(discrete=FALSE) +\n    ggtitle(\"Thinned intensity per site\")\nprint(p)\n\np <- ggplot(ppd_df, aes(x, y, fill=l)) + \n    geom_tile() +\n    scale_fill_viridis(discrete=FALSE) +\n    ggtitle(\"Intensity per site\")\nprint(p)\np <- ggplot(ppd_df, aes(x, y, fill=b)) + \n    geom_tile() +\n    scale_fill_viridis(discrete=FALSE) +\n    ggtitle(\"Bias per site\")\nprint(p)\n```\n:::\n",
    "supporting": [
      "poisson-processes_presence-only_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}