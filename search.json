[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Thinned Nonhomogenous Poisson Processes for Presence-Only data",
    "section": "",
    "text": "Welcome\nThis is a blog post by Max Savery. Please reach out to max.savery@ugent.be with any questions",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Welcome!\nThis is a blog post/discussion about using Nonhomogenous Poisson Point Processes for modelling Presence-Only data. This topic became a central unit of my PhD research, and so I wanted to condense the most important information about this data/model in a nice quarto document. Then I realized I could just make this into a blog post, so that is why it is here, now. In this document I discuss models for presence-only data, namely the Thinned Nonhomogenous Poisson Process (NHPP) and also, briefly, the Log Gaussian Cox Process (LGCP). The initial planned layout is as follows: Introduce presence-only data, Poisson Process models and the data generating process. Then, proceed to fit a Thinned Nonhomogenous Poisson Process in the Bayesian programming language Stan on simulated presence-only data. Finally, fit a LGCP for comparison.\nThe rest of this document follows the above description. Please reach out to max.savery@ugent.be with any questions",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "po_data.html",
    "href": "po_data.html",
    "title": "2  Presence-only data",
    "section": "",
    "text": "I’ll first quickly describe Presence-only data. It’s described in many other places, so I’ll be brief. First and foremost, it’s the sort of data you collect conveniently. Imagine walking in a field, looking for butterflies of a specific species. You walk randomly around, meandering so to speak. Whenever you find a butterfly of the species that you are interested in, you jot down the coordinates of your observation in your butterfly-observation moleskin journal. Maybe you also include some other information, such as the time or the temperature or type of foliage you stood upon when you saw the aforementioned butterfly.\nNow you go home, and look at your notebook with a number of “presences” and coordinates written down. What you have is point data: a set of locations in continuous space where you made a discrete observation of a positive presence. You didn’t walk around with any meaningful planned structure and you didn’t record “absences”, i.e., your personal failing to detect a butterfly. If you had more structure in your life and if you recorded absences during while walking in a more structured, planned fashion, well, than that would be a different kind of data. But that is not what you have.\nThat set of points you have with coordinates is what is described as Presence-only data (referred to as PO data).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Presence-only data</span>"
    ]
  },
  {
    "objectID": "theory.html",
    "href": "theory.html",
    "title": "3  Poisson process theory",
    "section": "",
    "text": "Remember the PO data. Now let’s think about how to model it. The points you collected, they actually can be described with a very specific mathematical structure, called a Nonhomogeneuous Poisson Process (NHPP). I’ll describe the mathematical details of it now.\nThe NHPP describes a set of points \\(S\\) where \\(\\{s_i\\} \\subseteq D\\) with intensity \\(\\lambda\\). Given an area or quadrat called subregion \\(A\\) of \\(D\\) of size \\(|A|\\) centered around site \\(s_i\\), we can model the number of individuals, \\(N_{s}(A)\\) occurring at that site with a poisson distribution. This distribution will have mean \\[\n\\Lambda(A) = \\int_A \\lambda(s)ds.\n\\]\nTypically, we model the intensity with a log-linear function, for example with parameters \\(\\alpha\\) and \\(\\beta\\), \\[\n\\lambda(s) = \\exp[\\alpha + \\beta'x(s)],\n\\] so that the expected number of observations in region A is now \\[\n\\Lambda(A) = \\int_A\\exp[\\alpha + \\beta'x(s)]ds.\n\\] We quickly notice that this integral is intractable and we will need a method of numerical integration. Indeed, this will be discussed and explored here.\nWe now define the likelihood for the NHPP (Banerjee, 2014, p214): \\[\nf(s_1, s_2...s_n|N(D)=n) = \\prod_i\\frac{\\lambda(s_i)}{(\\lambda(D))^n},\n\\] and the joint density will be \\[\nf(s_1, s_2...s_n,N(D)=n) = \\prod_i\\frac{\\lambda(s_i)}{(\\lambda(D))^n}\\left[(\\lambda(D))^n\\frac{\\exp(-\\lambda(D))}{n!}\\right],\n\\] where we can see the second term on the right corresponds to the poisson likelihood for the number of total observations in space \\(D\\). The likelihood will then be \\[\nL(\\lambda(s)|s_1, s_2,...,s_n) = \\prod_i\\lambda(s_i)\\frac{\\exp(-\\lambda(D))}{n!}.\n\\]\nNotice that we are still working with the points \\(s_i\\). Consider, however, that we are also going to be dealing with environmental data with environmental covariates (remember the notes you took about the temperature??), which in many cases are going to be discretized at some resolution inherently. For example, satellite data only occurs at the resolution of the raster image. Therefore, we can think of the NHPP as a continuous limit of a discretized set of conditionally independent Poisson random variables, where the NHPP emerges as the discretization grows smaller, i.e., \\(\\lambda(ds), ds\\to 0\\). In fact, using this continuous limit is how the likelihood for the NHPP can be derived (Banerjee, 2014. p203-4). Considering that we will be working with covariates in a discretized space, the continuous framework ends up being less practically useful in a sense. However, there are various frameworks for thinking about this type of problem. I have described it quite simply so I can move on. The point being, I will now show how to move from the continuous setting to the discrete.\nTo move between the continuous and discrete versions of this likelihood by partitioning \\(D\\) into a grid of \\(c\\) cells and taking the poisson likelihood: \\[\\begin{equation}\n\\label{eq:pp-l}\nL(\\lambda) = \\prod_c(\\lambda(A_i))^{N(A_i)}\\exp(-\\lambda(A_i)).\n\\end{equation}\\] where \\(N(A_i)\\) is the number of points occuring in cell \\(A_i\\). Noticing that we can sum all the exponents, we get \\[\nL(\\lambda) = \\prod_i(\\lambda(A_i))^{N(A_i)}\\exp(-[\\lambda(A_1) + \\lambda(A_2) + \\cdots+\\lambda(A_c)) = \\prod_i(\\lambda(A_i))^{N(A_i)}\\exp(-\\lambda(D)],\n\\] where \\(N(A_i) \\in \\{0,1\\}\\) as \\(|A_c|\\to 0\\). The term on the right indeed reduces to \\(\\ref{eq:pp-l}\\). We can then safely work with the Poisson likelihood given our grid is fine enough.\nWe still have, however, one issue. We originally defined the mean of the of region \\(A\\) according to the properties of the NHPP, \\[\n\\Lambda(A) = \\int_A\\lambda(s)ds= \\int_A\\exp[\\alpha + \\beta'x(s)]ds.\n\\] Suddenly we are saying that we can just work with the intensity for region \\(A\\) without clarifying what we should do about the integral over the functional for intensity. Unfortunately, while we may want the integral over \\(s\\), we don’t have information at the resolution of \\(ds\\). All we have are covariates \\(\\beta'X\\). That is, \\[\n\\Lambda(A) = \\int_A\\exp[\\alpha + \\beta'x(s)]ds \\approx |A|\\exp[\\alpha + \\beta'x].\n\\] Notice that we no longer have \\(x(s)\\). We assume the covariate information is relative constant over region \\(A\\). If this doesn’t hold, our approximation will not be correct. But, given that it is all the information we have, the assumption is built into any modelling procedure and is less an issue with the Poisson Process and more just an issue of covariate resolution, which is a common modelling issue in spatial-temporal statistics. As is stated in Banerjee, 2014 (p216) “In the absence of finer covariate resolution, we cannot do better with regard to the ecological fallacy.”\nFor the demonstration here, we assume \\(|A|=1\\). However, it is important to include the area to scale the parameters appropriately, as the parameters themselves are invariant to the scale of the data. In a GLM, we call this an offset.\nGiven this approximation and scaling assumption, we now have \\[\n\\Lambda(A) = \\int_A\\exp[\\alpha + \\beta'x(s)]ds \\approx |A|\\exp[\\alpha + \\beta'x] = \\frac{|D|}{c}\\exp[\\alpha + \\beta'x]\n\\] where \\(c\\) is the number of cells and \\(|D|\\) is our total area. Then the counts in subregion \\(A_i\\) will be distributed as \\[\nN(A_i) \\sim Poisson\\left(\\frac{|D|}{c}\\exp[\\alpha + \\beta'x_i]\\right) = Poisson\\left(\\exp[\\alpha + \\beta'x_i]\\right)\n\\] if the number of cells is equal to the area \\((|A| = 1)\\). Based on this, we end up with the log-likelihood for our entire presence-only dataset: \\[\nl(\\alpha, \\beta) = \\sum_{i\\in B}N(A_i)(\\alpha + \\beta'x_i) - \\frac{|D|}{c}\\sum_{i\\in B}\\exp[\\alpha + \\beta'x_i] - \\sum_{i\\in B}\\log(N(A_i))!\n\\] where \\(i\\in B\\) refers to the set of background points \\(B={1,2,...,c}\\). It is necessary to use the entire set of background points in order to appropriately estimate the integral (D) = _D(s)ds from the original NHPP likelihood.\nWith all of that, we have defined our model and the likelihood that we can use for PO data, and we have roughly established the discretized Poisson approximation to the NHPP.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Poisson process theory</span>"
    ]
  },
  {
    "objectID": "data_generation.html",
    "href": "data_generation.html",
    "title": "4  Presence-only data generation theory",
    "section": "",
    "text": "We now know about presence-only data and we know some theoretical basics about using a NHPP to model such data. There’s one last technical thing to discuss before we can get to actually fitting the model to the data. Often, when we are working with a new data type, it is very helpful to work in a simulated environment first. That way we can compare our models under an environment we completely control. However, to do so, we have to have be able to generate data according to the process we are interested in. Often, we can just use a marginal distribution to do so. But in the case of the NHPP, it’s not as straightforward. So we will describe how to simulate some presence-only data, in preparation to fit a nonhomogeneous poisson process model.\nTo generate data, let’s use the following ingredients: Each point observation \\(i\\) is associated with a site \\(s_i\\) area \\(D\\). For each location \\(s_i\\), there are associated covariates \\(x_i = x(s_i)\\) and \\(z_i = z(s_i)\\). And at each location, \\(s_i\\) represents the centroid of a quadrat \\(A_i\\). This is an essential assumption in our model, where we will use the Poisson likelihood to for the number of counts in each quadrat. At site \\(A_i\\) we observe counts \\(N(A_i)\\). This is described in Fithian (2015).\nReferencing our initial theoretical outline, we need to simulate points with likelihood \\[\n\\prod_i\\lambda(s_i)\\frac{\\exp(-\\lambda(D))}{n!}.\n\\] In the case of the homogeneous point process case, we can simulate \\(n\\) observations simply from \\[\nN(D) \\sim Poisson(\\lambda|D|),\n\\] where in the homogeneous case \\(\\lambda(s)=\\lambda\\), and is thus constant over \\(D\\). Then we distribute the \\(n\\) uniformly over \\(D\\).\nIn the case of the Nonhomogeneous Poisson Process, we can do the same, drawing \\(n\\) now from \\[\nN(D) \\sim Poisson(\\lambda(D)) = Poisson(\\int_D\\lambda(s)ds),\n\\] and then distributing their locations over \\(D\\). Instead of uniformly distributing them however, they are placed according to the distribution \\(\\frac{\\lambda(s)}{\\lambda(D)}\\). That is interesting. It is quite different than how we usually simulate non-spatial data.\nEDIT FROM HERE:\nOkay, so there are two issues here. How do we evaluate that integral over \\(D\\) both relating to the evaluation of the integral for the expected number of observations. Remember that we are using as intensity \\[\n\\lambda(A_i) = \\int_A\\lambda(s)ds = \\int_A\\exp[\\alpha + \\beta'x(s)]ds = \\exp[\\alpha + \\beta'x_i],\n\\] because we only have information \\(X_i\\) for quadrat \\(A_i\\) centered around \\(s_i\\). Using the \\(x_i\\) we have, both \\(\\lambda(A)\\) and \\(\\lambda(D)\\) will be approximations at the covariate resolution, using the assumption that each \\(x_i\\) is constant over \\(A_i\\). This being the case, then for each observation, we will use the same distribution as the other points that fall into the same \\(A_i\\). That is, multiple points will have the same distribution \\(\\frac{\\exp[\\alpha + \\beta'x_i]}{\\sum_i \\exp[\\alpha + \\beta'x_i]}\\). This goes somewhat against the idea of distributing the points in continuous space. Of course, we can uniformly distribute the points within each \\(A_i\\). If \\(x_i\\) is constant across \\(A_i\\), this is ok.\nHowever, if we cannot assume this, using the continuous probability distribution for each point \\((\\lambda(s)/\\lambda(D))\\) is necessary to avoid the integral of \\(\\lambda(A)\\). We can do so in a way that avoids computing \\(\\lambda(D)\\) as well (Banerjee, 2014. p220), by computing \\(\\lambda_{max}\\), simulating \\(n\\) from \\(N(D) \\sim Poisson(\\lambda_{max}|D|)\\), and the rejecting some of these points by \\(\\lambda(s_i)/\\lambda_{max}\\). In this way, the only thing we need is information at the level of \\(\\lambda(s_i\\)) and avoid \\(\\lambda(D)\\) and \\(\\lambda(A)\\). However, we are still limited by our covariate resolution. Essentially, the assumptions we make about the grid will actually effect the way we simulate data. For example, when the only covariate data we have is that which is associated with the quadrat we are working with, then to simulate the data, each point realization will be created in aggregate, that is, a draw from a poisson distribution characterized by an functional of intensity dependent only on the covariates.\nIf our covariates exist on a finer scale than the quadrat, we can simulate the point locations more finely as well and take the approach of Banerjee. However, if our covariates only exist at the level of the aggregated quadrat, then we assume that the intensity is constant across our region \\(A\\). It is the reasonable to simulate the points with Poisson draws and homogeneously distributing them across the space. So note that we take the route of simulation depending on our available information. If we only have \\(x_i\\) for each quadrat, directly generating the points as a poisson random variable is a good option because it avoids any complicated integrals. But if we have finer covariates than at the quadrat level we are working with, then thinning the process down with the approach of Banerjee and others is better because it also avoids the integrals and has finer resolution.\nWe must also take into account the bias associated with the observation of species with true intensity \\(\\lambda(s)\\). In order to account for the bias caused by collecting presence-only observations, the NHPP points are also thinned by the sampling bias \\(b\\). \\[\n\\lambda(s) = \\theta(s)b(s) = \\exp[\\alpha + \\beta'x(s) + \\gamma + \\delta'z(s)].\n\\]\nThere are a few things be aware of during the data gerating process with the NHPP. The area of the total space, \\(|D|\\), the area of each site (quadrat) \\(|A_i|\\), and the number of cells \\(c\\) that \\(D\\) is discretized into. The area of \\(A\\) depends on the number of discretizations (or cells): \\(|A_i| = |D|/c\\). The intensity will be scaled by this factor \\(|D|/c\\), so that as the area of \\(|A|\\) becomes smaller, the intensity converges to that of a “true” continuous poisson process with its center at \\(A_i\\). We also assume that the intensity is constant over \\(A\\), so that we can indeed scale the intensity by the area factor, instead of needing to integrate over \\(A\\). This assumption is in contrast to assuming that we model the instensity directly as \\(\\Lambda(A_i) = \\exp(\\beta'X)\\) (versus \\(\\Lambda(A_i) = \\frac{|D|}{c}\\exp(\\beta'X)\\)).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Presence-only data generation theory</span>"
    ]
  },
  {
    "objectID": "poisson-processes_presence-only.html",
    "href": "poisson-processes_presence-only.html",
    "title": "5  Data generation and Modelling",
    "section": "",
    "text": "5.1 Implementation\nThe previous sections discussed the theoretical and practical aspects of data generation. We now proceed to implement the data generation procedure. We first generate observations at each site and create plots of observed intensities, bias, and counts. We then fit a Poisson Point Process to the generated data.\nWe create a ring-shaped environment based on two correlated covariates and generate PO points accordingly.\narea_D &lt;- 100\nk &lt;- 20\nsites &lt;- k^2\n# Parameter values somewhere around Fithian 2015.\nalpha &lt;- -2 \nbeta &lt;- 2\ngamma &lt;- -2\ndelta &lt;- 0.5\nparams &lt;- list(alpha=alpha, beta=beta, gamma=gamma, delta=delta)\naux_cor &lt;- 0.7\ngrid_points &lt;- get_sampling_surface_donut(k)\n\n\n\n\n\n\n\ngrid_points &lt;- get_bias_surface_correlated(grid_points, aux_cor)\n\n[1] \"Correlation between X and Z\"\n         [,1]     [,2]\n[1,] 1.000000 0.829426\n[2,] 0.829426 1.000000\n\n\n\n\n\n\n\n\nsites &lt;- sites/4\ngrid_points &lt;- grid_points %&gt;% dplyr::filter(x&lt;(k/2)+1, y&lt;(k/2)+1)\n# Filter for only a quarter of the grid.\n# If we filter, we need to change the total sites as well\n# Need distances for LGCP\ndistance_mat &lt;- as.matrix(dist(grid_points[,1:2]))\ndata_reps &lt;- 1\ncorr_matrix &lt;- specify_corr(grid_points[,1:2])\ngp_bool &lt;- F\nsim_data &lt;- generate_ppp_data_r(grid_points, params, sites, data_reps, corr_matrix, gp_bool, area_D)\nY_positive_indices &lt;- which(sim_data$Y&gt;0)\nstopifnot(sum(sim_data$Y[Y_positive_indices]) == nrow(sim_data$coordinates))\nsim_data$Y_coords\n\n          x         y\n1  8.184528  3.314122\n2  8.519476  3.299914\n3  8.875100  2.612041\n4  9.840287  2.653246\n5  9.799889  2.554560\n6  5.629357  3.976328\n7  5.448773  5.301057\n8  4.592880  4.571052\n9  4.603478  5.246731\n10 5.199578  4.655172\n11 5.300851  5.094858\n12 3.764719  5.662555\n13 3.721151  6.254604\n14 2.853911  6.937763\n15 2.602734  9.284120\n16 2.620600 10.233323\n\ngrid_points[Y_positive_indices,]\n\n    x  y    aux_x     aux_z\n28  8  3 1.747454 1.7236321\n29  9  3 2.257839 3.0395881\n30 10  3 2.326348 1.0075442\n36  6  4 1.921817 0.8408047\n45  5  5 2.257839 1.6271448\n54  4  6 1.921817 1.2798297\n63  3  7 1.045421 0.4903408\n83  3  9 2.257839 2.5652340\n93  3 10 2.326348 2.4339077\nThe above output shows the exact location of the generated points and the associated \\(x\\) and \\(z\\) covariate values.\n# Use the final generation iteration\np &lt;- ggplot(grid_points, aes(x, y, fill=sim_data$lambda[data_reps,])) + \n  geom_tile() +\n  scale_fill_viridis(discrete=FALSE) +\n  ggtitle(\"Generated intensity per site\")\nprint(p)\n\n\n\n\n\n\n\np &lt;- ggplot(grid_points, aes(x, y, fill=sim_data$bias[data_reps,])) + \n  geom_tile() +\n  scale_fill_viridis(discrete=FALSE) +\n  ggtitle(\"Generated bias per site\")\nprint(p)\n\n\n\n\n\n\n\nthinned_intensity &lt;- sim_data$lambda[data_reps,]*sim_data$bias[data_reps,]\np &lt;- ggplot() +\n  geom_tile(grid_points, mapping=aes(x, y, fill=thinned_intensity, width=1, height=1), alpha=.6) + \n  scale_fill_viridis(discrete=FALSE, name=\"L*b\") +\n  ggtitle(\"Generated thinning per site, including generated (or observed) individuals\") +\n  geom_point(data=sim_data$Y_coords, mapping=aes(x=x, y=y), size=3, col=\"white\") +\n  theme(panel.grid.minor = element_line(colour=\"white\")) +\n  scale_y_continuous(breaks = seq(0, 20, 1)) +\n  scale_x_continuous(breaks = seq(0, 20, 1)) \nprint(p)\nThe above plots show the PO points and the ring-shaped surface.\nNext we fit the Thinned Nonhomogenous Poisson Process model in Stan. We can make a few observations about this model. First, examine the likelihood: \\[L(\\lambda|Y) = \\prod^{N}_{i=1}\\frac{\\lambda_i^{n_i}\\exp[-\\lambda_i]}{n_i!}\\]. This is made using the assumption that the counts at sites \\(s_i\\) are distributed following a poisson process with mean \\(\\lambda\\). This assumption allows for a simpler specification of the likelihood, since we can use the poisson distribution. The setting of \\(|A|=1\\) allows us to make this assumption, as we do not need to multiple the intensity parameter \\(\\lambda\\) by the area in the model.\nThe next sections of code fitting and diagnosing the poisson process model in Stan, using the generated data.\nmodel_string &lt;- stan_poisson_process\nwrite(model_string, model_path)\n# data_reps is 1 here so just use the first index\ndata = list(N = sites, X = grid_points$aux_x, Z = grid_points$aux_z, y = sim_data$Y[data_reps,])\nmodel &lt;- cmdstan_model(model_path)\nfit &lt;- model$sample(data=data, seed=13, chains=3, iter_sampling=2000, iter_warmup=200)\nThe above code fits the table. We next examine the fit.\nfit$summary(variables=c('alpha', 'beta', 'gamma', 'delta'))\n\n# A tibble: 4 × 10\n  variable   mean median    sd   mad      q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 alpha    -2.84  -3.00  7.24  7.17  -14.6   9.34   1.00    2041.    2117.\n2 beta      3.11   3.04  0.835 0.800   1.86  4.57   1.00    2169.    2222.\n3 gamma    -2.99  -3.03  7.19  7.20  -15.0   8.72   1.00    2079.    2254.\n4 delta    -0.262 -0.252 0.366 0.367  -0.877 0.332  1.00    2655.    2438.\nFrom the table above, we can observe that the parameters \\(\\beta\\) and \\(\\delta\\) have been recovered. However, due to the correlation of \\(\\alpha\\) and \\(\\gamma\\), only the sum \\(\\alpha+\\gamma\\) can be correctly identified. Our “true” sum of was equal to -4, and we can see the estimate here is \\(~4.7\\), which is reasonable given we incorporate priors and are estimating correlated parameters. The correlation between the two can be observed in the scatter plots below, as well as in the wide confidence interval of the posterior, relative to the other parameters.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Simulation and modelling</span>"
    ]
  },
  {
    "objectID": "poisson-processes_presence-only.html#diagnostics",
    "href": "poisson-processes_presence-only.html#diagnostics",
    "title": "5  Data generation and Modelling",
    "section": "5.2 Diagnostics",
    "text": "5.2 Diagnostics\nHere we check goodness of fit and Posterior Predictive Checks.\n\nparam_vec &lt;- c('alpha', 'gamma', 'beta', 'delta')\nposterior &lt;- fit$draws(variables=param_vec)\ncolor_scheme_set(\"mix-blue-pink\")\np_trace &lt;- mcmc_trace(posterior,  pars = c(\"alpha\", \"beta\", \"gamma\", \"delta\"),\n                      facet_args = list(nrow = 2, labeller = label_parsed))\nprint(p_trace + facet_text(size = 15))\n\n\n\n\n\n\n\nplot_title &lt;- ggtitle(paste(\"Posterior distributions, with means and 90% interval\"))\np_post &lt;- mcmc_areas(posterior,  prob = 0.9, point_est=\"mean\", regex_pars = c(\"alpha\", \"beta\")) + plot_title\nprint(p_post)\n\n\n\n\n\n\n\nplot_title &lt;- ggtitle(paste(\"Posterior distributions, with means and 90% interval\"))\np_post &lt;- mcmc_areas(posterior,  prob = 0.9, point_est=\"mean\", regex_pars = c(\"gamma\", \"delta\")) + plot_title\nprint(p_post)\n\n\n\n\n\n\n\nmcmc_intervals(posterior)\n\n\n\n\n\n\n\n\nImportantly, note the wide credible intervals on \\(\\alpha\\) and \\(\\gamma\\). Why? See the scatter plots below.\n\nplot_title &lt;- ggtitle(\"Parameter posterior sample correlation\")\np_post &lt;- mcmc_pairs(posterior)\nprint(p_post)\n\n\n\n\n\n\n\nmcmc_scatter(posterior, pars=c('alpha','gamma'))\n\n\n\n\n\n\n\n\nThere is near perfect correlation between \\(\\alpha\\) and \\(\\gamma\\), as expected.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Simulation and modelling</span>"
    ]
  },
  {
    "objectID": "poisson-processes_presence-only.html#ppd",
    "href": "poisson-processes_presence-only.html#ppd",
    "title": "5  Data generation and Modelling",
    "section": "5.3 PPD",
    "text": "5.3 PPD\nWe next look at the Posterior Predictive Distribution (PPD) of the observations per site. That is, we plot the generated counts \\(Y_i\\) for site \\(s_i\\) on the sampling surface (grid).\nThe PPD can be expressed as the samples from the predictive distribution of \\(Y\\) taking into account the uncertainty surrounding the parameters. The PPD can be written as \\[\np(\\tilde{y}|Y) = \\int p(\\tilde{y}|\\lambda, b, Y)p(\\lambda, b|Y) dbd\\lambda\n\\]\nsuch that the posterior uncertainty of the parameters is integrated out.\nBelow we plot the generated values of \\(\\tilde{y}\\) for each site. Additionally, the generated quantities of intensity, bias, and intensity*bias ($b) per site are also plotted on the grid.\n\ngenerated_vars &lt;- c('lambda_bias', 'lambda_rep', 'b_rep', 'y_rep')\nlambda_thinned &lt;- fit$summary(variables=generated_vars[1])$mean\nlambda_rep &lt;- fit$summary(variables=generated_vars[2])$mean\nbias_rep &lt;- fit$summary(variables=generated_vars[3])$mean\ny_ppd &lt;- fit$summary(variables=generated_vars[4])$mean\nppd_df &lt;- data.frame(x=grid_points$x, y=grid_points$y, y_rep=y_ppd, lt=lambda_thinned, l=lambda_rep, b=bias_rep)\nhead(ppd_df)\n\n  x y        y_rep           lt        l        b\n1 1 1 0.0001666667 7.199373e-05 1697.153 23958337\n2 2 1 0.0001666667 8.195836e-05 1702.332 32896510\n3 3 1 0.0000000000 8.400412e-05 1758.149 32704473\n4 4 1 0.0000000000 9.987791e-05 2119.245 33369782\n5 5 1 0.0001666667 1.251540e-04 3645.357 18129194\n6 6 1 0.0001666667 2.742875e-04 8627.968 26331935\n\n\nIt is important to notice that while the thinned intensity is ostensibly realistic in estimation, the intensity and bias are not. This is due to the specification of the Thinned Poisson Process \\[\\lambda^* = \\lambda(s)b(s)\\] such that \\[\\lambda^* = \\exp\\left[\\alpha + \\beta*X(s) + \\gamma + \\delta*Z(s)\\right]\\]\nWe can then see the generated data in the sampling surface:\n\np &lt;- ggplot(ppd_df, aes(x, y, fill=y_rep)) + \n    geom_tile() +\n    scale_fill_viridis(discrete=FALSE) +\n    ggtitle(\"PPD counts per site\")\nprint(p)\n\n\n\n\n\n\n\np &lt;- ggplot(ppd_df, aes(x, y, fill=lt)) + \n    geom_tile() +\n    scale_fill_viridis(discrete=FALSE) +\n    ggtitle(\"Thinned intensity per site\")\nprint(p)\n\n\n\n\n\n\n\np &lt;- ggplot(ppd_df, aes(x, y, fill=l)) + \n    geom_tile() +\n    scale_fill_viridis(discrete=FALSE) +\n    ggtitle(\"Intensity per site\")\nprint(p)\n\n\n\n\n\n\n\np &lt;- ggplot(ppd_df, aes(x, y, fill=b)) + \n    geom_tile() +\n    scale_fill_viridis(discrete=FALSE) +\n    ggtitle(\"Bias per site\")\nprint(p)\n\n\n\n\n\n\n\n\nThe prediction of PO observatiosn at each site is realistic, as is the thinned intensity. But alone, the intensity and bias predictive disitributions are poorly estimated.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Simulation and modelling</span>"
    ]
  },
  {
    "objectID": "poisson-processes_presence-only.html#correlation-between-coefficients",
    "href": "poisson-processes_presence-only.html#correlation-between-coefficients",
    "title": "5  Data generation and Modelling",
    "section": "5.4 Correlation between coefficients",
    "text": "5.4 Correlation between coefficients\nIn the above plots, it seems that the bias and unthinned intensity are behaving strangely, even though the thinned intensity is generated as we would expect. This is a manifestation of the completely correlated parameters describing the behavior of the intensity and the bias. We can notice that the generated bias and intensity alone are both really large. The issue with the large values appears this way because we are taking the mean of the posterior distribution as our point estimate and showing that in the plot of the sites above. However, when we generate the thinned intensity, we multiply each draw from the posterior predictive chain together. So even though the mean of the posterior chain is high, the individual draws of the bias and intensity inversely fluctuate around each other, due the high correlation between the two parameters.\nWe take a look at the posterior intervals of the estimates to get a little better idea of this.\n\nplot_title &lt;- ggtitle(paste(\"Posterior distributions, with means and 90% interval\"))\np_post &lt;- mcmc_intervals(fit$draws(),  prob = 0.9, point_est=\"mean\", regex_pars = generated_vars[1]) + plot_title\nprint(p_post)\n\n\n\n\n\n\n\n\nThe intervals shown above are for the thinned bias. These are relatively reasonable, as stated.\n\np_post &lt;- mcmc_intervals(fit$draws(),  prob = 0.9, point_est=\"mean\", regex_pars = generated_vars[2]) + plot_title\nprint(p_post)\n\nWarning: Removed 29 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nHowever, the intervals for the intensity (unthinned) are unrealistic, due to the unindentifiable parameters.\n\np_post &lt;- mcmc_intervals(fit$draws(),  prob = 0.9, point_est=\"mean\", regex_pars = generated_vars[3]) + plot_title\nprint(p_post)\n\nWarning: Removed 100 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nAnd finally, the intervals of the generated bias is also not correct.\nThen we look at the intervals and histograms of the posterior chains for just a few chains.\n\n# The gen parameters for just a few of the sites\ngen_lambda &lt;- c('lambda_rep[1]', 'lambda_rep[10]', 'lambda_rep[20]')\ngen_lb &lt;- c('lambda_bias[1]', 'lambda_bias[10]', 'lambda_bias[20]')\ngen_b &lt;- c('b_rep[1]', 'b_rep[10]', 'b_rep[20]')\np_post &lt;- mcmc_intervals(fit$draws(),  prob = 0.9, point_est=\"mean\", pars = gen_lambda) + plot_title\nprint(p_post)\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\np_post &lt;- mcmc_intervals(fit$draws(),  prob = 0.9, point_est=\"mean\", pars = gen_b) + plot_title\nprint(p_post)\n\nWarning: Removed 3 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\np_post &lt;- mcmc_intervals(fit$draws(),  prob = 0.9, point_est=\"mean\", pars = gen_lb) + plot_title\nprint(p_post)\n\n\n\n\n\n\n\np_post &lt;- mcmc_hist(fit$draws(), pars = gen_lambda) \nprint(p_post)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\np_post &lt;- mcmc_hist(fit$draws(), pars = gen_b)\nprint(p_post)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\np_post &lt;- mcmc_hist(fit$draws(), pars = gen_lb) \nprint(p_post)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n# Look at counts for each site\n#gen_y &lt;- c(\"y_rep[1]\", \"y_rep[10]\", \"y_rep[20]\")\n#p_post &lt;- mcmc_hist(fit$draws(), pars=gen_y)\n#print(p_post)\n\nFrom this we notice that the intervals for some of the generated lambdas and biases are wide, but not for the thinned values. But even for those that have wide intervals, nearly all of the chain values in the histograms are the same. This is because it’s only a few values that cause the mean to be biased, due to the correlation of the parameters.\nThe take away is that the mean for the generated quantities is very skewed by the occassional large value. This high skew is caused by the correlation between b and l, since these parameters are not estimable between themselves. Therefore, we arrive at reasonable estimates for b*l since this is draw specific (where when one is high the other is low), but the overall mean for either b or l over multiple draws cannot be trusted. This is clear from the histograms of \\(\\lambda\\) and the bias, where most of the values take on very low values, but do fluctuate higher. And when one goes higher, the other goes lower, which I’d like to check a specific example of, for one draw.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Simulation and modelling</span>"
    ]
  },
  {
    "objectID": "poisson-processes_presence-only.html#diagnostics-1",
    "href": "poisson-processes_presence-only.html#diagnostics-1",
    "title": "5  Data generation and Modelling",
    "section": "5.5 Diagnostics",
    "text": "5.5 Diagnostics\nFinally, it is possible to look at a variety of NUTS diagnostics metrics. I am not going to go into much detail here.\n\nnuts_fit &lt;- nuts_params(fit)\nmcmc_parcoord(fit$  draws(), np=nuts_fit)\n\n\n\n\n\n\n\nmcmc_nuts_divergence(nuts_fit, log_posterior(fit))\n\n\n\n\n\n\n\n\nThe parcord plot shows 1 line per iteration of the sampler. We can see the values each sampled parameter takes on, which allows us to notice the extent that correlation has on the sampled values. If there are any divergences in the sampling, these will show up in red and help us to pick out the parameters that may be contributing. to this.\nThere are no divergences so we don’t need to worry.\n\nmcmc_rhat(rhat(fit))\n\nWarning: Dropped 19 NAs from 'new_rhat(rhat)'.\n\n\n\n\n\n\n\n\nmcmc_neff(neff_ratio(fit))\n\nWarning: Dropped 19 NAs from 'new_neff_ratio(ratio)'.\n\n\n\n\n\n\n\n\n#mcmc_nuts_energy(nuts_fit, binwidth=1/2)\n#mcmc_nuts_acceptance(nuts_fit,  log_posterior(fit))\n\nR hat, which compares between and within chain estimates, looks good.\n“neff” is the effective sample size estimates the number of independent samples, after accounting for autocorrelation in the chains. The figure here shows there are a reasonable number of independent samples, though the threshold is somewhat arbitrary.\n\nmcmc_acf(fit$draws(), pars=c('alpha', 'beta', 'gamma', 'delta'))\n\n\n\n\n\n\n\n\nAutocorrelation in the chains looks well-handled.\n\nfit$cmdstan_diagnose()\n\nProcessing csv files: C:/Users/msavery/AppData/Local/Temp/RtmpeMUL7c/thinned_poisson_process-202405031018-1-818f2d.csv, C:/Users/msavery/AppData/Local/Temp/RtmpeMUL7c/thinned_poisson_process-202405031018-2-818f2d.csv, C:/Users/msavery/AppData/Local/Temp/RtmpeMUL7c/thinned_poisson_process-202405031018-3-818f2d.csv\n\nChecking sampler transitions treedepth.\nTreedepth satisfactory for all transitions.\n\nChecking sampler transitions for divergences.\nNo divergent transitions found.\n\nChecking E-BFMI - sampler transitions HMC potential energy.\nE-BFMI satisfactory.\n\nEffective sample size satisfactory.\n\nSplit R-hat values satisfactory all parameters.\n\nProcessing complete, no problems detected.\n\n# Leave loo out.\n#loo_results &lt;- fit$loo(variables=\"lp__\", cores=4)\n#print(loo_results)\n\nIt appears we have no additional problems in the models according the the Stan diagnostics.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Simulation and modelling</span>"
    ]
  },
  {
    "objectID": "poisson-processes_presence-only.html#lgcp",
    "href": "poisson-processes_presence-only.html#lgcp",
    "title": "5  Data generation and Modelling",
    "section": "5.6 LGCP",
    "text": "5.6 LGCP\nWe next move on to incorporating spatial correlation. To do so, we fit a Log Gaussian Cox Process (LGCP). Where before we specified the intensity as \\[\n\\lambda(s) = \\theta(s)b(s) = \\exp[\\alpha + \\beta'x(s) + \\gamma + \\delta'z(s)]\n\\] we now add a Gaussian process to incorporate spatial correlation between sites \\[\n\\lambda(s) = \\theta(s)b(s) = \\exp[\\alpha + \\beta'x(s) + \\gamma + \\delta'z(s) + w(s)]\n\\] so that the expected value for quadrat \\(A\\) will be the integral over the quadrat: \\[\n\\Delta(A) = \\int_A\\lambda(s)ds = \\int_A \\exp[\\alpha + \\beta'x(s) + \\gamma + \\delta'z(s) + w(s)]ds\n\\] We make the same approximations as before based on our limited covariate resolution.\nWhile the topic of LGCPs on its own is quite interesting and deserves its own treatment, for now we examine its behavior only as an adjustment to the NHPP. In a future post I will discuss approximations to the LGCP, which are quite important as fitting the spatial correlation matrix is computationally intensive as the number of locations increases. If we want to apply the LGCP to developing optimal designs (another future topic), we will need an approximation.\nFor now, I proceed with fitting the model in Stan.\n\nmodel_string &lt;- log_gaussian_cox_process\nwrite(model_string, model_path)\n# Here we need to use the distance matrix\ndata = list(N = sites, X = grid_points$aux_x, Z = grid_points$aux_z, y = sim_data$Y[data_reps,], D = distance_mat)\nmodel &lt;- cmdstan_model(model_path)\nlgcp_fit &lt;- model$sample(data=data, seed=13, chains=3, iter_sampling=2000, iter_warmup=200, thin = 10)\n\nHaving fit the LGCP we can examine the parameters {r) #| label: lgcp-params #| eval: true lgcp_fit$summary(variables=c('alpha', 'beta', 'gamma', 'delta')) And then we look at the posterior diagnostics.\n\nposterior &lt;- lgcp_fit$draws()\ncolor_scheme_set(\"mix-blue-pink\")\np_trace &lt;- mcmc_trace(posterior,  pars = c(\"alpha\", \"beta\", \"gamma\", \"delta\"),\n                      facet_args = list(nrow = 2, labeller = label_parsed))\nprint(p_trace + facet_text(size = 15))\n\n\n\n\n\n\n\n\nThen look at the posterior intervals\n\nplot_title &lt;- ggtitle(paste(\"Posterior distributions, with means and 90% interval\"))\np_post &lt;- mcmc_areas(posterior,  prob = 0.9, point_est=\"mean\", regex_pars = c(\"alpha\", \"beta\")) + plot_title\nprint(p_post)\n\n\n\n\n\n\n\nplot_title &lt;- ggtitle(paste(\"Posterior distributions, with means and 90% interval\"))\np_post &lt;- mcmc_areas(posterior,  prob = 0.9, point_est=\"mean\", regex_pars = c(\"gamma\", \"delta\")) + plot_title\nprint(p_post)\n\n\n\n\n\n\n\nmcmc_intervals(posterior, pars=c('alpha', 'beta', 'gamma', 'delta'))\n\n\n\n\n\n\n\n#mcmc_hist(fit$draws(), pars = c('alpha', 'beta', 'gamma', 'delta')) \nplot_title &lt;- ggtitle(\"Parameter posterior sample correlation\")\np_post &lt;- mcmc_pairs(posterior, pars=c('alpha', 'beta', 'gamma', 'delta'))\nprint(p_post)\n\n\n\n\n\n\n\n\n\nmcmc_scatter(posterior, pars=c('alpha','gamma'))\n\n\n\n\n\n\n\n\nFinally look at the posterior predictions for \\(\\lambda\\), \\(b\\), and \\(\\lambda\\cdot b\\).\n\ngenerated_vars &lt;- c('lambda_bias', 'lambda_rep', 'b_rep', 'y_rep')\nlambda_thinned &lt;- lgcp_fit$summary(variables=generated_vars[1])$mean\nlambda_rep &lt;- lgcp_fit$summary(variables=generated_vars[2])$mean\nbias_rep &lt;- lgcp_fit$summary(variables=generated_vars[3])$mean\ny_ppd &lt;- lgcp_fit$summary(variables=generated_vars[4])$mean\nppd_df &lt;- data.frame(x=grid_points$x, y=grid_points$y, y_rep=y_ppd, lt=lambda_thinned, l=lambda_rep, b=bias_rep)\n\nWe can then check the generated data:\n\np &lt;- ggplot(ppd_df, aes(x, y, fill=y_rep)) + \n    geom_tile() +\n    scale_fill_viridis(discrete=FALSE) +\n    ggtitle(\"PPD counts per site\")\nprint(p)\n\n\n\n\n\n\n\np &lt;- ggplot(ppd_df, aes(x, y, fill=lt)) + \n    geom_tile() +\n    scale_fill_viridis(discrete=FALSE) +\n    ggtitle(\"Thinned intensity per site\")\nprint(p)\n\n\n\n\n\n\n\np &lt;- ggplot(ppd_df, aes(x, y, fill=l)) + \n    geom_tile() +\n    scale_fill_viridis(discrete=FALSE) +\n    ggtitle(\"Intensity per site\")\nprint(p)\n\n\n\n\n\n\n\np &lt;- ggplot(ppd_df, aes(x, y, fill=b)) + \n    geom_tile() +\n    scale_fill_viridis(discrete=FALSE) +\n    ggtitle(\"Bias per site\")\nprint(p)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Simulation and modelling</span>"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "6  Conclusion",
    "section": "",
    "text": "To summarize, I first described the necessary theory to understand the Poisson Point Process and its estimation. Importantly, by discretizing our region, we can approximate the process using the Poisson distribution to model the counts in each of the discrete sites. I then fit the Poisson Point Process in Stan and took some time to discuss the unidentifiability of the thinned intensity parameterization. Finally, I fit a Log-Gaussian Cox Process and briefly discussed the fit. It is important to use the LGCP as soon as you need to incorporate spatial or temporal correlation between the points. However, to efficiently fit this model, approximations are required. This is a well-studied topic and in a future post I will take a look at some of these.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  }
]